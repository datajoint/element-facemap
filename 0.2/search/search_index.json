{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "", "title": "Element Facemap", "text": "<p>DataJoint Element for modeling neural activity based on orofacial tracking using Facemap. This Element supports facemap analysis using distinct keypoints on the mouse face, and computing the singular value decomposition and pupil tracking. DataJoint Elements collectively standardize and automate data collection and analysis for neuroscience experiments. Each Element is a modular pipeline for data storage and processing with corresponding database tables that can be combined with other Elements to assemble a fully functional pipeline.</p>"}, {"location": "#experiment-flowchart", "title": "Experiment Flowchart", "text": ""}, {"location": "#data-pipeline-diagram", "title": "Data Pipeline Diagram", "text": "<ul> <li>We have designed two variations of the pipeline to handle different use cases. Displayed above is the latest <code>facemap_inference</code> schema. Details on all of the <code>facemap</code> schemas can be found in the Data Pipeline documentation page.</li> </ul>"}, {"location": "#getting-started", "title": "Getting Started", "text": "<ul> <li>Please fork the repository</li> </ul> <ul> <li>Clone the repository to your computer<pre><code>git clone https://github.com/&lt;enter_github_username&gt;/element-facemap.git\n</code></pre> </li> </ul> <ul> <li>Install with <code>pip</code><pre><code>pip install -e .\n</code></pre> </li> </ul> <ul> <li>Data Pipeline - Pipeline and table descriptions</li> </ul> <ul> <li>Tutorials - Start building your data pipeline</li> </ul> <ul> <li>Code Repository</li> </ul>"}, {"location": "#support", "title": "Support", "text": "<ul> <li>If you need help getting started or run into any errors, please contact our team by email at support@datajoint.com.</li> </ul>"}, {"location": "CHANGELOG/", "title": "Changelog", "text": "<p>Observes Semantic Versioning standard and Keep a Changelog convention.</p>"}, {"location": "CHANGELOG/#022-2024-05-12", "title": "[0.2.2] - 2024-05-12", "text": "<ul> <li>Fix - Fix docs by updating <code>mkdocs</code></li> <li>Update - Remove the <code>pypi_release</code> from <code>release.yaml</code></li> </ul>"}, {"location": "CHANGELOG/#021-2024-05-12", "title": "[0.2.1] - 2024-05-12", "text": "<ul> <li>Fix - Links in README</li> <li>Update - README and docs</li> </ul>"}, {"location": "CHANGELOG/#020-2023-11-29", "title": "0.2.0 - 2023-11-29", "text": "<ul> <li>Add - Inference module as <code>facemap_inference.py</code></li> <li>Add - DevContainer for codespaces</li> <li>Add - <code>tutorial_pipeline.py</code></li> <li>Add - 60 min tutorial for <code>facemap_inference</code> using Jupyter Notebooks</li> <li>Update - General improvements to increase consistency with other DataJoint Elements</li> </ul>"}, {"location": "CHANGELOG/#017-2023-10-18", "title": "0.1.7 - 2023-10-18", "text": "<ul> <li>Fix - Index <code>mot_Sv</code> and <code>movSv</code> by <code>roi_no</code> in <code>FacialSignal</code> </li> </ul>"}, {"location": "CHANGELOG/#016-2023-05-22", "title": "0.1.6 - 2023-05-22", "text": "<ul> <li>Add - Facemap citation</li> <li>Update - mkdocs.yaml</li> </ul>"}, {"location": "CHANGELOG/#015-2023-05-11", "title": "0.1.5 - 2023-05-11", "text": "<ul> <li>Fix - <code>.ipynb</code> dark mode output for all notebooks.</li> <li>Fix - Remove <code>GOOGLE_ANALYTICS_KEY</code> from <code>u24_element_release_call.yml</code>.</li> <li>Remove - <code>.staging workflows</code> from CI/CD</li> </ul>"}, {"location": "CHANGELOG/#014-2023-04-28", "title": "0.1.4 - 2023-04-28", "text": "<ul> <li>Fix - <code>.ipynb</code> output in tutorials is not visible in dark mode.</li> </ul>"}, {"location": "CHANGELOG/#013-2022-10-11", "title": "0.1.3 - 2022-10-11", "text": "<ul> <li>Add - CICD workflows for PyPI release</li> <li>Fix - lagging motion index</li> </ul>"}, {"location": "CHANGELOG/#012-2022-05-17", "title": "0.1.2 - 2022-05-17", "text": "<ul> <li>Fix - Recording duration attribute name</li> </ul>"}, {"location": "CHANGELOG/#011-2022-05-13", "title": "0.1.1 - 2022-05-13", "text": "<ul> <li>Fix - any -&gt; np.any</li> </ul>"}, {"location": "CHANGELOG/#010-2022-05-10", "title": "0.1.0 - 2022-05-10", "text": "<ul> <li>Add - Adopt black formatting into code base</li> </ul>"}, {"location": "CHANGELOG/#001-2022-03-29", "title": "0.0.1 - 2022-03-29", "text": "<ul> <li>Add - Multiple file processing</li> <li>Add - Support for avi video files</li> <li>Add - Stores left singular vector, right singular vector, and the non-zero elements of   the diagonal matrix</li> <li>Add - Output directory inferral, if unspecified.</li> </ul>"}, {"location": "citation/", "title": "Citation", "text": "<p>If your work uses the following resources, please cite the respective manuscript and/or Research Resource Identifier (RRID):</p> <ul> <li>DataJoint Element Facemap - Version 0.2.2      + Yatsenko D, Nguyen T, Shen S, Gunalan K, Turner CA, Guzman R, Sasaki M, Sitonic D,      Reimer J, Walker EY, Tolias AS. DataJoint Elements: Data Workflows for      Neurophysiology. bioRxiv. 2021 Jan 1. doi: https://doi.org/10.1101/2021.03.30.437358<p>+ RRID:SCR_021894</p> </li> </ul> <ul> <li>Facemap      + Manuscripts</li> </ul>"}, {"location": "concepts/", "title": "Concepts", "text": ""}, {"location": "concepts/#facial-motion-tracking", "title": "Facial Motion Tracking", "text": "<p>Neuroscience research often involves understanding the connections between neural activity and subject behavior. Many mammals, including mice, exhibit facial expressions that provide insights into their emotional and neuronal states<sup>1</sup>. Facemap<sup>2</sup><sup>,</sup><sup>3</sup> is an open-source software developed to streamline the quantification of facial movements such as whisker, eye, and pupil motions through computer vision techniques.</p> <p>In its initial versions, Facemap empowered researchers to identify regions of interest (ROIs) on the animal's face as either rectangles or ellipses, using example frames from video recordings. The software utilized singular value decomposition on these ROIs across both raw movie frames and frame-wise difference values to detect motion. This process, grounded in principle component analysis, yielded a set of components that represent distinct facial features. To achieve optimal results, it was recommended to use video recordings from fixed cameras to ensure that all captured motions were attributable to the subject's facial movements.<sup>2</sup></p> <p>The latest iteration of Facemap introduces the ability to track keypoints across the animal's face. This feature marks a departure from solely relying on predefined ROIs, allowing for more dynamic and precise analysis of facial expressions and movements.<sup>3</sup></p> <ul> <li>KeyPoints Detection: Facemap now employs cutting-edge machine learning algorithms to automatically detect and track specific facial landmarks, such as the tips of whiskers, the corners of the eyes, and the edges of the mouth. This approach enables a finer-grained analysis of facial expressions, enhancing the software's utility in behavioral neuroscience research.</li> </ul> <ul> <li>Dynamic Tracking: Unlike the static ROIs, keypoints move with the subject across frames. This dynamic tracking ensures that more subtle facial movements are captured, providing richer datasets for analysis.</li> </ul>"}, {"location": "concepts/#element-features", "title": "Element Features", "text": "<p>Through our interviews and direct collaborations, we identified the common motifs to create Element Facemap.</p> <p>Major features include:</p> <ul> <li>Ingestion and storage of input video metadata.</li> <li>Queueing and triggering of Facemap analysis on multiple sessions.</li> <li>Ingestion of analysis outcomes as motion and video principle components.</li> <li>Ingestion of analysis outcomes from inference of facial keypoints.</li> </ul> <ol> <li> <p>Dolensek, N., Gehrlach, D. A., Klein, A. S., &amp; Gogolla, N. (2020). Facial expressions of emotion states and their neuronal correlates in mice. Science, 368(6486), 89-94.\u00a0\u21a9</p> </li> <li> <p>Syeda, A., Zhong, L., Tung, R., Long, W., Pachitariu, M., &amp; Stringer, C. (2024). Facemap: a framework for modeling neural activity based on orofacial tracking. Nature Neuroscience, 27(1), 187-195.\u00a0\u21a9\u21a9</p> </li> <li> <p>Stringer, C., Pachitariu, M., Steinmetz, N., Reddy, C. B., Carandini, M., &amp; Harris, K. D. (2019). Spontaneous behaviors drive multidimensional, brainwide activity. Science, 364(6437), eaav7893.\u00a0\u21a9\u21a9</p> </li> </ol>"}, {"location": "partnerships/", "title": "Key Partnerships", "text": "<p>Element Facemap was developed in collaboration with Hui Chen Lu's Lab at Indiana University Bloomington. </p> <p>Our team also works with the Facemap developers to promote integration and interoperability between Facemap and the DataJoint Element Facemap.</p>"}, {"location": "pipeline/", "title": "Data Pipeline", "text": "<p>Each node in the following diagram represents the analysis code in the pipeline and the corresponding table in the database.  Within the pipeline, Element Facemap connects to upstream Elements including Lab, Animal, and Session. For more  detailed documentation on each table, see the API docs for the respective schemas.</p> <p>The element is composed of two main schemas, <code>facial_behavior_estimation</code> and <code>facemap_inference</code>. The <code>facial_behavior_estimation</code> schema is designed to handle the analysis and ingestion Facemap's SVD analysis for pupil and ROI tracking. The <code>facemap_inference</code> schema is designed to handle the analysis and ingestion of Facemap's pose estimation and tracking key points on the mouse face.</p>"}, {"location": "pipeline/#diagrams", "title": "Diagrams", "text": ""}, {"location": "pipeline/#facial_behavior_estimation-module", "title": "<code>facial_behavior_estimation</code> module", "text": "<ul> <li>The <code>facial_behavior_estimation</code> schema is designed to handle the analysis and ingestion Facemap's SVD analysis for pupil and ROI tracking. </li> </ul>"}, {"location": "pipeline/#facemap_inference-module", "title": "<code>facemap_inference</code> module", "text": "<ul> <li>The <code>facemap_inference</code> schema is designed to handle the analysis and ingestion of Facemap's pose estimation and tracking key points on the mouse face. </li> </ul>"}, {"location": "pipeline/#table-descriptions", "title": "Table Descriptions", "text": ""}, {"location": "pipeline/#lab-schema", "title": "<code>lab</code> schema", "text": "<ul> <li>For further details see the lab schema API docs</li> </ul> Table Description Device Scanner metadata"}, {"location": "pipeline/#subject-schema", "title": "<code>subject</code> schema", "text": "<ul> <li>Although not required, most choose to connect the <code>Session</code> table to a <code>Subject</code>   table.</li> </ul> <ul> <li>For further details see the subject schema API docs</li> </ul> Table Description Subject Basic information of the research subject."}, {"location": "pipeline/#session-schema", "title": "<code>session</code> schema", "text": "<ul> <li>For further details see the session schema API docs</li> </ul> Table Description Session Unique experimental session identifier."}, {"location": "pipeline/#facial_behavior_estimation-schema", "title": "<code>facial_behavior_estimation</code> schema", "text": "<ul> <li>For further details see the facial_behavior_estimation schema API docs</li> </ul> Table Description VideoRecording Video(s) from one recording session, for Facial Motion Tracking. RecordingInfo Information extracted from video file. FacemapTask Staging table for pairing of recording and Facemap parameters before processing. FacemapProcessing Automated table to execute Facemap with inputs from FacemapTask. FacialSignal Results of the Facemap analysis. FacialSignal.Region Region properties. FacialSignal.MotionSVD Components of the SVD from motion video. FacialSignal.MovieSVD Components of the SVD from movie video. FacialSignal.Summary Average frames for movie and motion videos."}, {"location": "pipeline/#facemap_inference-schema", "title": "<code>facemap_inference</code> schema", "text": "<ul> <li>For further details see the facemap_inference schema API docs</li> </ul> Table Description BodyPart Body parts tracked by Facemap models. FacemapModel Trained models stored for facial pose inference. FacemapModel.BodyPart Body parts associated with a given model. FacemapModel.File File paths to facemap models. FacemapInferenceTask Staging table for pairing of video recordings and Facemap model before running inference. FacemapInference Automated table to execute Facemap with inputs from FacemapInferenceTask. FacemapInference.BodyPartPosition Position of individual body parts."}, {"location": "roadmap/", "title": "Roadmap", "text": "<p>Further development of this Element is community driven. Upon user requests and based on guidance from the Scientific Steering Group we will continue adding features to this  Element.</p>"}, {"location": "api/element_facemap/facemap_inference/", "title": "facemap_inference.py", "text": ""}, {"location": "api/element_facemap/facemap_inference/#element_facemap.facemap_inference.activate", "title": "<code>activate(facemap_model_schema_name, fbe_schema_name=None, *, create_schema=True, create_tables=True, linking_module=None)</code>", "text": "<p>Activate schema.</p> <p>Parameters:</p> Name Type Description Default <code>facemap_model_schema_name</code> <code>str</code> <p>Schema name on the database server to activate the <code>facemap_inference</code> schema of element-facemap</p> required <code>fbe_schema_name</code> <code>str</code> <p>Schema name on the database server to activate the 'facial_behavioral_estimation</p> <code>None</code> <code>create_schema</code> <code>bool</code> <p>When True (default), create schema in the database if it does not yet exist.</p> <code>True</code> <code>create_tables</code> <code>bool</code> <p>When True (default), create tables in the database if they do not yet exist.</p> <code>True</code> <code>linking_module</code> <code>str</code> <p>A module name or a module containing the required dependencies to activate the <code>facial_behavior_estimation</code> module:</p> <code>None</code> <p>Dependencies: Upstream tables:     + Session: A parent table to VideoRecording, identifying a recording session.     + Equipment: A parent table to VideoRecording, identifying video recording equipment.     + VideoRecording: A parent table to FacemapInferenceTask, identifying videos to be used in inference.</p> Source code in <code>element_facemap/facemap_inference.py</code> <pre><code>def activate(\n    facemap_model_schema_name: str,\n    fbe_schema_name: str = None,\n    *,\n    create_schema: bool = True,\n    create_tables: bool = True,\n    linking_module=None,\n):\n    \"\"\"Activate schema.\n\n    Args:\n        facemap_model_schema_name (str): Schema name on the database server to activate the\n            `facemap_inference` schema of element-facemap\n        fbe_schema_name (str):  Schema name on the database server to activate the 'facial_behavioral_estimation\n        create_schema (bool): When True (default), create schema in the database if it\n            does not yet exist.\n        create_tables (bool): When True (default), create tables in the database if\n            they do not yet exist.\n        linking_module (str): A module name or a module containing the required\n            dependencies to activate the `facial_behavior_estimation` module:\n\n    Dependencies:\n    Upstream tables:\n        + Session: A parent table to VideoRecording, identifying a recording session.\n        + Equipment: A parent table to VideoRecording, identifying video recording equipment.\n        + VideoRecording: A parent table to FacemapInferenceTask, identifying videos to be used in inference.\n    \"\"\"\n    if isinstance(linking_module, str):\n        linking_module = importlib.import_module(linking_module)\n    assert inspect.ismodule(\n        linking_module\n    ), \"The argument 'dependency' must be a module's name or a module\"\n    assert hasattr(\n        linking_module, \"get_facemap_root_data_dir\"\n    ), \"The linking module must specify a lookup function for a root data directory\"\n\n    global _linking_module\n    _linking_module = linking_module\n\n    fbe.activate(\n        fbe_schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        linking_module=linking_module,\n    )\n    schema.activate(\n        facemap_model_schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        add_objects=_linking_module.__dict__,\n    )\n</code></pre>"}, {"location": "api/element_facemap/facemap_inference/#element_facemap.facemap_inference.BodyPart", "title": "<code>BodyPart</code>", "text": "<p>               Bases: <code>Lookup</code></p> <p>Body parts tracked by Facemap models.</p> <p>Attributes:</p> Name Type Description <code>body_part</code> <code>str</code> <p>Body part short name.</p> <code>body_part_description</code> <code>str</code> <p>Detailed body part description.</p> Source code in <code>element_facemap/facemap_inference.py</code> <pre><code>@schema\nclass BodyPart(dj.Lookup):\n    \"\"\"Body parts tracked by Facemap models.\n\n    Attributes:\n        body_part (str): Body part short name.\n        body_part_description (str, optional): Detailed body part description.\n\n    \"\"\"\n\n    definition = \"\"\"\n    body_part                : varchar(32)\n    ---\n    body_part_description='' : varchar(1000)\n    \"\"\"\n\n    # Facemap Default BodyPart list\n    contents = [\n        (\"eye(back)\", \"\"),\n        (\"eye(bottom)\", \"\"),\n        (\"eye(front)\", \"\"),\n        (\"eye(top)\", \"\"),\n        (\"lowerlip\", \"\"),\n        (\"mouth\", \"\"),\n        (\"nose(bottom)\", \"\"),\n        (\"nose(r)\", \"\"),\n        (\"nose(tip)\", \"\"),\n        (\"nose(top)\", \"\"),\n        (\"nosebridge\", \"\"),\n        (\"paw\", \"\"),\n        (\"whisker(I)\", \"\"),\n        (\"whisker(III)\", \"\"),\n        (\"whisker(II)\", \"\"),\n    ]\n</code></pre>"}, {"location": "api/element_facemap/facemap_inference/#element_facemap.facemap_inference.FacemapModel", "title": "<code>FacemapModel</code>", "text": "<p>               Bases: <code>Manual</code></p> <p>Trained Models stored for facial pose inference.</p> <p>Attributes:</p> Name Type Description <code>model_id</code> <code>int</code> <p>User specified unique model ID associated with a model.</p> <code>model_name</code> <code>str</code> <p>Name of model.</p> <code>model_description</code> <code>str</code> <p>Detailed model description.</p> Source code in <code>element_facemap/facemap_inference.py</code> <pre><code>@schema\nclass FacemapModel(dj.Manual):\n    \"\"\"Trained Models stored for facial pose inference.\n\n    Attributes:\n        model_id (int): User specified unique model ID associated with a model.\n        model_name (str): Name of model.\n        model_description (str, optional): Detailed model description.\n    \"\"\"\n\n    definition = \"\"\"\n    model_id                  : int              # user assigned ID associated with a unique model\n    ---\n    model_name                : varchar(64)      # name of model \n    model_description=''      : varchar(1000)    # optional model description\n    \"\"\"\n\n    class BodyPart(dj.Part):\n        \"\"\"Body parts associated with a given model\n\n        Attributes:\n            body_part (str): Body part name.\n            body_part_description (str): Detailed body part description.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        -&gt; BodyPart\n        \"\"\"\n\n    class File(dj.Part):\n        \"\"\"Relative paths of facemap models with respect to facemap_root_data_dir\n\n        Attributes:\n            FacemapModel (foreign key): Primary key from FacemapModel.\n            model_file (attach): Facemap model file.\n\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        ---\n        model_file: attach      # model file attachment. Stored as binary in database.\n        \"\"\"\n\n    @classmethod\n    def insert_new_model(\n        cls,\n        model_id: int,\n        model_name: str,\n        model_description: str,\n        full_model_path: str,\n    ):\n        \"\"\"Insert a new model into the FacemapModel table and relevant part tables.\n\n        Args:\n            model_id (int): User specified unique model ID associated with a model.\n            model_name (str): Name of model.\n            model_description (str): Detailed model description.\n            full_model_path (str): Full path to the model file.\n        \"\"\"\n        cls.insert1(\n            dict(\n                model_id=model_id,\n                model_name=model_name,\n                model_description=model_description,\n            )\n        )\n\n        cls.BodyPart.insert(\n            [\n                dict(\n                    model_id=model_id,\n                    body_part=part,\n                )\n                for part in BodyPart.fetch(\"body_part\")\n            ]\n        )\n\n        cls.File.insert1(\n            dict(\n                model_id=model_id,\n                model_file=full_model_path,\n            ),\n        )\n</code></pre>"}, {"location": "api/element_facemap/facemap_inference/#element_facemap.facemap_inference.FacemapModel.BodyPart", "title": "<code>BodyPart</code>", "text": "<p>               Bases: <code>Part</code></p> <p>Body parts associated with a given model</p> <p>Attributes:</p> Name Type Description <code>body_part</code> <code>str</code> <p>Body part name.</p> <code>body_part_description</code> <code>str</code> <p>Detailed body part description.</p> Source code in <code>element_facemap/facemap_inference.py</code> <pre><code>class BodyPart(dj.Part):\n    \"\"\"Body parts associated with a given model\n\n    Attributes:\n        body_part (str): Body part name.\n        body_part_description (str): Detailed body part description.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    -&gt; BodyPart\n    \"\"\"\n</code></pre>"}, {"location": "api/element_facemap/facemap_inference/#element_facemap.facemap_inference.FacemapModel.File", "title": "<code>File</code>", "text": "<p>               Bases: <code>Part</code></p> <p>Relative paths of facemap models with respect to facemap_root_data_dir</p> <p>Attributes:</p> Name Type Description <code>FacemapModel</code> <code>foreign key</code> <p>Primary key from FacemapModel.</p> <code>model_file</code> <code>attach</code> <p>Facemap model file.</p> Source code in <code>element_facemap/facemap_inference.py</code> <pre><code>class File(dj.Part):\n    \"\"\"Relative paths of facemap models with respect to facemap_root_data_dir\n\n    Attributes:\n        FacemapModel (foreign key): Primary key from FacemapModel.\n        model_file (attach): Facemap model file.\n\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    ---\n    model_file: attach      # model file attachment. Stored as binary in database.\n    \"\"\"\n</code></pre>"}, {"location": "api/element_facemap/facemap_inference/#element_facemap.facemap_inference.FacemapModel.insert_new_model", "title": "<code>insert_new_model(model_id, model_name, model_description, full_model_path)</code>  <code>classmethod</code>", "text": "<p>Insert a new model into the FacemapModel table and relevant part tables.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>int</code> <p>User specified unique model ID associated with a model.</p> required <code>model_name</code> <code>str</code> <p>Name of model.</p> required <code>model_description</code> <code>str</code> <p>Detailed model description.</p> required <code>full_model_path</code> <code>str</code> <p>Full path to the model file.</p> required Source code in <code>element_facemap/facemap_inference.py</code> <pre><code>@classmethod\ndef insert_new_model(\n    cls,\n    model_id: int,\n    model_name: str,\n    model_description: str,\n    full_model_path: str,\n):\n    \"\"\"Insert a new model into the FacemapModel table and relevant part tables.\n\n    Args:\n        model_id (int): User specified unique model ID associated with a model.\n        model_name (str): Name of model.\n        model_description (str): Detailed model description.\n        full_model_path (str): Full path to the model file.\n    \"\"\"\n    cls.insert1(\n        dict(\n            model_id=model_id,\n            model_name=model_name,\n            model_description=model_description,\n        )\n    )\n\n    cls.BodyPart.insert(\n        [\n            dict(\n                model_id=model_id,\n                body_part=part,\n            )\n            for part in BodyPart.fetch(\"body_part\")\n        ]\n    )\n\n    cls.File.insert1(\n        dict(\n            model_id=model_id,\n            model_file=full_model_path,\n        ),\n    )\n</code></pre>"}, {"location": "api/element_facemap/facemap_inference/#element_facemap.facemap_inference.FacemapInferenceTask", "title": "<code>FacemapInferenceTask</code>", "text": "<p>               Bases: <code>Manual</code></p> <p>A pairing of video recordings and Facemap model.</p> <p>Attributes:</p> Name Type Description <code>fbe.VideoRecording</code> <code>foreign key</code> <p>Primary key from VideoRecording table.</p> <code>FacemapModel</code> <code>foreign key</code> <p>Primary key from FacemapModel table.</p> <code>facemap_inference_output_dir</code> <code>str</code> <p>output dir storing the results of pose analysis.</p> <code>task_mode</code> <code>str</code> <p>One of 'load' (load computed analysis results) or 'trigger' (trigger computation).</p> <code>bbox</code> <code>longblob, nullable) </code> <p>Bounding box for cropping the video [x1, x2, y1, y2]. If not set, entire frame is used.</p> <code>task_description</code> <code>str, optional) </code> <p>Task description.</p> Source code in <code>element_facemap/facemap_inference.py</code> <pre><code>@schema\nclass FacemapInferenceTask(dj.Manual):\n    \"\"\"A pairing of video recordings and Facemap model.\n\n    Attributes:\n        fbe.VideoRecording (foreign key): Primary key from VideoRecording table.\n        FacemapModel (foreign key): Primary key from FacemapModel table.\n        facemap_inference_output_dir (str): output dir storing the results of pose analysis.\n        task_mode (str): One of 'load' (load computed analysis results) or 'trigger' (trigger computation).\n        bbox (longblob, nullable) : Bounding box for cropping the video [x1, x2, y1, y2]. If not set, entire frame is used.\n        task_description (str, optional) : Task description.\n    \"\"\"\n\n    definition = \"\"\"\n    # Staging table for pairing of recording and Facemap model.\n    -&gt; fbe.VideoRecording\n    -&gt; FacemapModel\n    ---\n    facemap_inference_output_dir    : varchar(255)  # Output directory of processed results of Facemap inference analysis relative to root directory.\n    task_description=''             : varchar(128)  # Optional. Additional task description\n    task_mode='load'                : enum('load', 'trigger') \n    bbox=null                       : longblob  # list containing bounding box for cropping the video [x1, x2, y1, y2]\n    \"\"\"\n\n    @classmethod\n    def infer_output_dir(cls, key, relative=False, mkdir=False):\n        \"\"\"Infer an output directory for an entry in FacemapInferenceTask table.\n\n        Args:\n            key (dict): Primary key from the FacemapInferenceTask table.\n            relative (bool, optional): If True, facemap_inference_output_dir is returned\n            relative to facemap_root_dir. Defaults to True.\n            mkdir (bool, optional): If True, create facemap_inference_output_dir. Defaults to True.\n\n        Returns:\n            dir (str): A default output directory for inference results (facemap_inference_output_dir\n                in FacemapInferenceTask) based on the following convention:\n                processed_dir / relative_video_dir / {facemap_recordingid}_{model_id}\n                e.g.: sub1/sess1/video_files/facemap_recording_id0_model0\n        \"\"\"\n        video_file = (fbe.VideoRecording.File &amp; key).fetch(\"file_path\", limit=1)[0]\n        video_dir = find_full_path(get_facemap_root_data_dir(), video_file).parent\n        root_dir = find_root_directory(get_facemap_root_data_dir(), video_dir)\n\n        processed_dir = Path(get_facemap_processed_data_dir())\n        output_dir = (\n            processed_dir\n            / video_dir.relative_to(root_dir)\n            / f\"facemap_recordingid{key['recording_id']}_model{key['model_id']}\"\n        )\n\n        if mkdir:\n            output_dir.mkdir(parents=True, exist_ok=True)\n\n        return output_dir.relative_to(processed_dir) if relative else output_dir\n\n    @classmethod\n    def generate(\n        cls,\n        key,\n        task_description: str = \"\",\n        task_mode: str = \"load\",\n        bbox: list = [],\n    ):\n        \"\"\"Generate a unique pose estimation task for each of the relative_video_paths\n\n        Args:\n            key (dict): Primary key from FacemapInferenceTask table\n                e.g.: {subject=\"sub1\",session_id=0,recording_id=0,model_id=0}\n            relative_video_paths (list): list of relative videos in VideoRecording.File table\n            task_mode (str, optional): 'load' or 'trigger. Defaults to 'load'.\n            bbox (list, optional): Bounding box for processing. Defaults to [].\n        \"\"\"\n        facemap_inference_output_dir = cls.infer_output_dir(key)\n\n        cls.insert1(\n            dict(\n                **key,\n                facemap_inference_output_dir=facemap_inference_output_dir,\n                task_description=task_description,\n                task_mode=task_mode,\n                bbox=bbox,\n            ),\n        )\n\n    insert_facemap_inference_task = generate\n</code></pre>"}, {"location": "api/element_facemap/facemap_inference/#element_facemap.facemap_inference.FacemapInferenceTask.infer_output_dir", "title": "<code>infer_output_dir(key, relative=False, mkdir=False)</code>  <code>classmethod</code>", "text": "<p>Infer an output directory for an entry in FacemapInferenceTask table.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>Primary key from the FacemapInferenceTask table.</p> required <code>relative</code> <code>bool</code> <p>If True, facemap_inference_output_dir is returned</p> <code>False</code> <code>mkdir</code> <code>bool</code> <p>If True, create facemap_inference_output_dir. Defaults to True.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dir</code> <code>str</code> <p>A default output directory for inference results (facemap_inference_output_dir in FacemapInferenceTask) based on the following convention: processed_dir / relative_video_dir / {facemap_recordingid}_{model_id} e.g.: sub1/sess1/video_files/facemap_recording_id0_model0</p> Source code in <code>element_facemap/facemap_inference.py</code> <pre><code>@classmethod\ndef infer_output_dir(cls, key, relative=False, mkdir=False):\n    \"\"\"Infer an output directory for an entry in FacemapInferenceTask table.\n\n    Args:\n        key (dict): Primary key from the FacemapInferenceTask table.\n        relative (bool, optional): If True, facemap_inference_output_dir is returned\n        relative to facemap_root_dir. Defaults to True.\n        mkdir (bool, optional): If True, create facemap_inference_output_dir. Defaults to True.\n\n    Returns:\n        dir (str): A default output directory for inference results (facemap_inference_output_dir\n            in FacemapInferenceTask) based on the following convention:\n            processed_dir / relative_video_dir / {facemap_recordingid}_{model_id}\n            e.g.: sub1/sess1/video_files/facemap_recording_id0_model0\n    \"\"\"\n    video_file = (fbe.VideoRecording.File &amp; key).fetch(\"file_path\", limit=1)[0]\n    video_dir = find_full_path(get_facemap_root_data_dir(), video_file).parent\n    root_dir = find_root_directory(get_facemap_root_data_dir(), video_dir)\n\n    processed_dir = Path(get_facemap_processed_data_dir())\n    output_dir = (\n        processed_dir\n        / video_dir.relative_to(root_dir)\n        / f\"facemap_recordingid{key['recording_id']}_model{key['model_id']}\"\n    )\n\n    if mkdir:\n        output_dir.mkdir(parents=True, exist_ok=True)\n\n    return output_dir.relative_to(processed_dir) if relative else output_dir\n</code></pre>"}, {"location": "api/element_facemap/facemap_inference/#element_facemap.facemap_inference.FacemapInferenceTask.generate", "title": "<code>generate(key, task_description='', task_mode='load', bbox=[])</code>  <code>classmethod</code>", "text": "<p>Generate a unique pose estimation task for each of the relative_video_paths</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>Primary key from FacemapInferenceTask table e.g.: {subject=\"sub1\",session_id=0,recording_id=0,model_id=0}</p> required <code>relative_video_paths</code> <code>list</code> <p>list of relative videos in VideoRecording.File table</p> required <code>task_mode</code> <code>str</code> <p>'load' or 'trigger. Defaults to 'load'.</p> <code>'load'</code> <code>bbox</code> <code>list</code> <p>Bounding box for processing. Defaults to [].</p> <code>[]</code> Source code in <code>element_facemap/facemap_inference.py</code> <pre><code>@classmethod\ndef generate(\n    cls,\n    key,\n    task_description: str = \"\",\n    task_mode: str = \"load\",\n    bbox: list = [],\n):\n    \"\"\"Generate a unique pose estimation task for each of the relative_video_paths\n\n    Args:\n        key (dict): Primary key from FacemapInferenceTask table\n            e.g.: {subject=\"sub1\",session_id=0,recording_id=0,model_id=0}\n        relative_video_paths (list): list of relative videos in VideoRecording.File table\n        task_mode (str, optional): 'load' or 'trigger. Defaults to 'load'.\n        bbox (list, optional): Bounding box for processing. Defaults to [].\n    \"\"\"\n    facemap_inference_output_dir = cls.infer_output_dir(key)\n\n    cls.insert1(\n        dict(\n            **key,\n            facemap_inference_output_dir=facemap_inference_output_dir,\n            task_description=task_description,\n            task_mode=task_mode,\n            bbox=bbox,\n        ),\n    )\n</code></pre>"}, {"location": "api/element_facemap/facemap_inference/#element_facemap.facemap_inference.FacemapInference", "title": "<code>FacemapInference</code>", "text": "<p>               Bases: <code>Computed</code></p> <p>Perform facemap pose estimation.</p> <p>Attributes:</p> Name Type Description <code>FacemapInferenceTask</code> <code>foreign key</code> <p>Primary key from FacemapInferenceTask.</p> <code>inference_completion_time</code> <code>datetime</code> <p>Inference completion datetime.</p> <code>inference_run_duration</code> <code>datetime</code> <p>Duration to inference completion.</p> <code>total_frame_count</code> <code>int</code> <p>Number of frames in all video files.</p> Source code in <code>element_facemap/facemap_inference.py</code> <pre><code>@schema\nclass FacemapInference(dj.Computed):\n    \"\"\"Perform facemap pose estimation.\n\n    Attributes:\n        FacemapInferenceTask (foreign key): Primary key from FacemapInferenceTask.\n        inference_completion_time (datetime): Inference completion datetime.\n        inference_run_duration (datetime): Duration to inference completion.\n        total_frame_count (int): Number of frames in all video files.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; FacemapInferenceTask\n    ---\n    inference_completion_time: datetime  # time of generation of this set of facemap results\n    inference_run_duration: float # seconds\n    total_frame_count: int          # frame count across all video files          \n    \"\"\"\n\n    class BodyPartPosition(dj.Part):\n        \"\"\"Position of individual body parts by frame index.\n\n        Attributes:\n            FacemapInference (foreign key): Primary key from FacemapInference.\n            FacemapModel.BodyPart (foreign key): Primary key from FacemapModel.BodyPart.\n            x_pos (longblob): X position.\n            y_pos (longblob): Y position.\n            likelihood (longblob): Model confidence.\"\"\"\n\n        definition = \"\"\" # uses facemap h5 output for body part position\n        -&gt; master\n        -&gt; FacemapModel.BodyPart\n        ---\n        x_pos       : longblob      # x position\n        y_pos       : longblob      # y position\n        likelihood  : longblob      # model evaluated likelihood\n        \"\"\"\n\n    def make(self, key):\n        \"\"\".populate() method will launch training for each FacemapInferenceTask\"\"\"\n        # ID model and directories\n        task_mode, output_dir = (FacemapInferenceTask &amp; key).fetch1(\n            \"task_mode\", \"facemap_inference_output_dir\"\n        )\n\n        if not output_dir:\n            output_dir = FacemapInferenceTask.infer_output_dir(\n                key, relative=True, mkdir=True\n            )\n            # update facemap_inference_output_dir\n            FacemapInferenceTask.update1(\n                {**key, \"facemap_inference_output_dir\": output_dir.as_posix()}\n            )\n\n        output_dir = find_full_path(fbe.get_facemap_root_data_dir(), output_dir)\n        video_files = (FacemapInferenceTask * fbe.VideoRecording.File &amp; key).fetch(\n            \"file_path\"\n        )\n\n        video_files = [\n            find_full_path(fbe.get_facemap_root_data_dir(), video_file)\n            for video_file in video_files\n        ]\n        vid_name = Path(video_files[0]).stem\n        facemap_result_path = output_dir / f\"{vid_name}_FacemapPose.h5\"\n        full_metadata_path = output_dir / f\"{vid_name}_FacemapPose_metadata.pkl\"\n\n        # Load or Trigger Facemap Pose Estimation Inference\n        if (\n            facemap_result_path.exists() &amp; full_metadata_path.exists()\n        ) or task_mode == \"load\":  # Load results and do not rerun processing\n            (\n                body_part_position_entry,\n                inference_duration,\n                total_frame_count,\n                creation_time,\n            ) = _load_facemap_results(key, facemap_result_path, full_metadata_path)\n            self.insert1(\n                {\n                    **key,\n                    \"inference_completion_time\": creation_time,\n                    \"inference_run_duration\": inference_duration,\n                    \"total_frame_count\": total_frame_count,\n                }\n            )\n            self.BodyPartPosition.insert(body_part_position_entry)\n            return\n\n        elif task_mode == \"trigger\":\n            from facemap.pose import pose as facemap_pose, model_loader\n\n            bbox = (FacemapInferenceTask &amp; key).fetch1(\"bbox\") or []\n\n            # Fetch model(.pt) file attachment to present working directory\n            facemap_model_name = (\n                FacemapModel.File &amp; f'model_id=\"{key[\"model_id\"]}\"'\n            ).fetch1(\"model_file\")\n\n            facemap_model_path = Path.cwd() / facemap_model_name\n            models_root_dir = model_loader.get_models_dir()\n\n            # Create Symbolic Links to raw video data files from outbox directory\n            video_symlinks = []\n            for video_file in video_files:\n                video_symlink = output_dir / video_file.name\n                if video_symlink.exists():\n                    video_symlink.unlink()\n                video_symlink.symlink_to(video_file)\n                video_symlinks.append(video_symlink.as_posix())\n\n            # copy this model file to the facemap model root directory (~/.facemap/models/)\n            shutil.copy(facemap_model_path, models_root_dir)\n\n            # Instantiate Pose object, with filenames specified as video files, and bounding specified in params\n            # Assumes GUI to be none as we are running CLI implementation\n            pose = facemap_pose.Pose(\n                filenames=[video_symlinks],\n                model_name=facemap_model_path.stem,\n                bbox=bbox,\n                bbox_set=bool(bbox),\n            )\n            pose.run()\n\n            (\n                body_part_position_entry,\n                inference_duration,\n                total_frame_count,\n                creation_time,\n            ) = _load_facemap_results(key, facemap_result_path, full_metadata_path)\n            self.insert1(\n                {\n                    **key,\n                    \"inference_completion_time\": creation_time,\n                    \"inference_run_duration\": inference_duration,\n                    \"total_frame_count\": total_frame_count,\n                }\n            )\n            self.BodyPartPosition.insert(body_part_position_entry)\n\n    @classmethod\n    def get_trajectory(cls, key: dict, body_parts: list = \"all\") -&gt; pd.DataFrame:\n        \"\"\"Returns a pandas dataframe of coordinates of the specified body_part(s)\n\n        Args:\n            key (dict): A DataJoint query specifying one FacemapInferenceEstimation entry.\n            body_parts (list, optional): Body parts as a list. If \"all\", all joints\n\n        Returns:\n            df: multi index pandas dataframe with Facemap model name, body_parts\n                and x/y coordinates of each body part for a camera_id, similar to\n                output of facemap inference data.\n        \"\"\"\n        model_name = (FacemapModel &amp; f'model_id={key[\"model_id\"]}').fetch1(\"model_name\")\n\n        if body_parts == \"all\":\n            body_parts = (cls.BodyPartPosition &amp; key).fetch(\"body_part\")\n        elif not isinstance(body_parts, list):\n            body_parts = list(body_parts)\n\n        df = None\n        for body_part in body_parts:\n            result_dict = (\n                cls.BodyPartPosition\n                &amp; {\"body_part\": body_part}\n                &amp; {\"recording_id\": key[\"recording_id\"]}\n                &amp; {\"session_id\": key[\"session_id\"]}\n            ).fetch(\"x_pos\", \"y_pos\", \"likelihood\", as_dict=True)[0]\n            x_pos = result_dict[\"x_pos\"].tolist()\n            y_pos = result_dict[\"y_pos\"].tolist()\n            likelihood = result_dict[\"likelihood\"].tolist()\n            a = np.vstack((x_pos, y_pos, likelihood))\n            a = a.T\n            pdindex = pd.MultiIndex.from_product(\n                [[model_name], [body_part], [\"x\", \"y\", \"likelihood\"]],\n                names=[\"model\", \"bodyparts\", \"coords\"],\n            )\n            frame = pd.DataFrame(a, columns=pdindex, index=range(0, a.shape[0]))\n            df = pd.concat([df, frame], axis=1)\n        return df\n</code></pre>"}, {"location": "api/element_facemap/facemap_inference/#element_facemap.facemap_inference.FacemapInference.BodyPartPosition", "title": "<code>BodyPartPosition</code>", "text": "<p>               Bases: <code>Part</code></p> <p>Position of individual body parts by frame index.</p> <p>Attributes:</p> Name Type Description <code>FacemapInference</code> <code>foreign key</code> <p>Primary key from FacemapInference.</p> <code>FacemapModel.BodyPart</code> <code>foreign key</code> <p>Primary key from FacemapModel.BodyPart.</p> <code>x_pos</code> <code>longblob</code> <p>X position.</p> <code>y_pos</code> <code>longblob</code> <p>Y position.</p> <code>likelihood</code> <code>longblob</code> <p>Model confidence.</p> Source code in <code>element_facemap/facemap_inference.py</code> <pre><code>class BodyPartPosition(dj.Part):\n    \"\"\"Position of individual body parts by frame index.\n\n    Attributes:\n        FacemapInference (foreign key): Primary key from FacemapInference.\n        FacemapModel.BodyPart (foreign key): Primary key from FacemapModel.BodyPart.\n        x_pos (longblob): X position.\n        y_pos (longblob): Y position.\n        likelihood (longblob): Model confidence.\"\"\"\n\n    definition = \"\"\" # uses facemap h5 output for body part position\n    -&gt; master\n    -&gt; FacemapModel.BodyPart\n    ---\n    x_pos       : longblob      # x position\n    y_pos       : longblob      # y position\n    likelihood  : longblob      # model evaluated likelihood\n    \"\"\"\n</code></pre>"}, {"location": "api/element_facemap/facemap_inference/#element_facemap.facemap_inference.FacemapInference.make", "title": "<code>make(key)</code>", "text": "<p>.populate() method will launch training for each FacemapInferenceTask</p> Source code in <code>element_facemap/facemap_inference.py</code> <pre><code>def make(self, key):\n    \"\"\".populate() method will launch training for each FacemapInferenceTask\"\"\"\n    # ID model and directories\n    task_mode, output_dir = (FacemapInferenceTask &amp; key).fetch1(\n        \"task_mode\", \"facemap_inference_output_dir\"\n    )\n\n    if not output_dir:\n        output_dir = FacemapInferenceTask.infer_output_dir(\n            key, relative=True, mkdir=True\n        )\n        # update facemap_inference_output_dir\n        FacemapInferenceTask.update1(\n            {**key, \"facemap_inference_output_dir\": output_dir.as_posix()}\n        )\n\n    output_dir = find_full_path(fbe.get_facemap_root_data_dir(), output_dir)\n    video_files = (FacemapInferenceTask * fbe.VideoRecording.File &amp; key).fetch(\n        \"file_path\"\n    )\n\n    video_files = [\n        find_full_path(fbe.get_facemap_root_data_dir(), video_file)\n        for video_file in video_files\n    ]\n    vid_name = Path(video_files[0]).stem\n    facemap_result_path = output_dir / f\"{vid_name}_FacemapPose.h5\"\n    full_metadata_path = output_dir / f\"{vid_name}_FacemapPose_metadata.pkl\"\n\n    # Load or Trigger Facemap Pose Estimation Inference\n    if (\n        facemap_result_path.exists() &amp; full_metadata_path.exists()\n    ) or task_mode == \"load\":  # Load results and do not rerun processing\n        (\n            body_part_position_entry,\n            inference_duration,\n            total_frame_count,\n            creation_time,\n        ) = _load_facemap_results(key, facemap_result_path, full_metadata_path)\n        self.insert1(\n            {\n                **key,\n                \"inference_completion_time\": creation_time,\n                \"inference_run_duration\": inference_duration,\n                \"total_frame_count\": total_frame_count,\n            }\n        )\n        self.BodyPartPosition.insert(body_part_position_entry)\n        return\n\n    elif task_mode == \"trigger\":\n        from facemap.pose import pose as facemap_pose, model_loader\n\n        bbox = (FacemapInferenceTask &amp; key).fetch1(\"bbox\") or []\n\n        # Fetch model(.pt) file attachment to present working directory\n        facemap_model_name = (\n            FacemapModel.File &amp; f'model_id=\"{key[\"model_id\"]}\"'\n        ).fetch1(\"model_file\")\n\n        facemap_model_path = Path.cwd() / facemap_model_name\n        models_root_dir = model_loader.get_models_dir()\n\n        # Create Symbolic Links to raw video data files from outbox directory\n        video_symlinks = []\n        for video_file in video_files:\n            video_symlink = output_dir / video_file.name\n            if video_symlink.exists():\n                video_symlink.unlink()\n            video_symlink.symlink_to(video_file)\n            video_symlinks.append(video_symlink.as_posix())\n\n        # copy this model file to the facemap model root directory (~/.facemap/models/)\n        shutil.copy(facemap_model_path, models_root_dir)\n\n        # Instantiate Pose object, with filenames specified as video files, and bounding specified in params\n        # Assumes GUI to be none as we are running CLI implementation\n        pose = facemap_pose.Pose(\n            filenames=[video_symlinks],\n            model_name=facemap_model_path.stem,\n            bbox=bbox,\n            bbox_set=bool(bbox),\n        )\n        pose.run()\n\n        (\n            body_part_position_entry,\n            inference_duration,\n            total_frame_count,\n            creation_time,\n        ) = _load_facemap_results(key, facemap_result_path, full_metadata_path)\n        self.insert1(\n            {\n                **key,\n                \"inference_completion_time\": creation_time,\n                \"inference_run_duration\": inference_duration,\n                \"total_frame_count\": total_frame_count,\n            }\n        )\n        self.BodyPartPosition.insert(body_part_position_entry)\n</code></pre>"}, {"location": "api/element_facemap/facemap_inference/#element_facemap.facemap_inference.FacemapInference.get_trajectory", "title": "<code>get_trajectory(key, body_parts='all')</code>  <code>classmethod</code>", "text": "<p>Returns a pandas dataframe of coordinates of the specified body_part(s)</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>A DataJoint query specifying one FacemapInferenceEstimation entry.</p> required <code>body_parts</code> <code>list</code> <p>Body parts as a list. If \"all\", all joints</p> <code>'all'</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>multi index pandas dataframe with Facemap model name, body_parts and x/y coordinates of each body part for a camera_id, similar to output of facemap inference data.</p> Source code in <code>element_facemap/facemap_inference.py</code> <pre><code>@classmethod\ndef get_trajectory(cls, key: dict, body_parts: list = \"all\") -&gt; pd.DataFrame:\n    \"\"\"Returns a pandas dataframe of coordinates of the specified body_part(s)\n\n    Args:\n        key (dict): A DataJoint query specifying one FacemapInferenceEstimation entry.\n        body_parts (list, optional): Body parts as a list. If \"all\", all joints\n\n    Returns:\n        df: multi index pandas dataframe with Facemap model name, body_parts\n            and x/y coordinates of each body part for a camera_id, similar to\n            output of facemap inference data.\n    \"\"\"\n    model_name = (FacemapModel &amp; f'model_id={key[\"model_id\"]}').fetch1(\"model_name\")\n\n    if body_parts == \"all\":\n        body_parts = (cls.BodyPartPosition &amp; key).fetch(\"body_part\")\n    elif not isinstance(body_parts, list):\n        body_parts = list(body_parts)\n\n    df = None\n    for body_part in body_parts:\n        result_dict = (\n            cls.BodyPartPosition\n            &amp; {\"body_part\": body_part}\n            &amp; {\"recording_id\": key[\"recording_id\"]}\n            &amp; {\"session_id\": key[\"session_id\"]}\n        ).fetch(\"x_pos\", \"y_pos\", \"likelihood\", as_dict=True)[0]\n        x_pos = result_dict[\"x_pos\"].tolist()\n        y_pos = result_dict[\"y_pos\"].tolist()\n        likelihood = result_dict[\"likelihood\"].tolist()\n        a = np.vstack((x_pos, y_pos, likelihood))\n        a = a.T\n        pdindex = pd.MultiIndex.from_product(\n            [[model_name], [body_part], [\"x\", \"y\", \"likelihood\"]],\n            names=[\"model\", \"bodyparts\", \"coords\"],\n        )\n        frame = pd.DataFrame(a, columns=pdindex, index=range(0, a.shape[0]))\n        df = pd.concat([df, frame], axis=1)\n    return df\n</code></pre>"}, {"location": "api/element_facemap/facial_behavior_estimation/", "title": "facial_behavior_estimation.py", "text": ""}, {"location": "api/element_facemap/facial_behavior_estimation/#element_facemap.facial_behavior_estimation.activate", "title": "<code>activate(facemap_schema_name, *, create_schema=True, create_tables=True, linking_module=None)</code>", "text": "<p>Activate schema.</p> <p>Parameters:</p> Name Type Description Default <code>facemap_schema_name</code> <code>str</code> <p>Schema name on the database server to activate the <code>facemap</code> element</p> required <code>create_schema</code> <code>bool</code> <p>When True (default), create schema in the database if it does not yet exist.</p> <code>True</code> <code>create_tables</code> <code>bool</code> <p>When True (default), create tables in the database if they do not yet exist.</p> <code>True</code> <code>linking_module</code> <code>str</code> <p>A module name or a module containing the required dependencies to activate the <code>facial_behavior_estimation</code> module:</p> <code>None</code> <p>Dependencies: Upstream tables:     + Session: A parent table to VideoRecording, identifying a recording session     + Equipment: A parent table to VideoRecording, identifying video recording equipment Functions:     + get_facemap_root_data_dir() -&gt; list         Retrieves the root data directory(s) with face recordings for all         subject/sessions. Returns a string for the full path to the root data directory.     + get_facemap_processed_data_dir(session_key: dict) -&gt; str         Optional function to retrieve the desired output directory         for Facemap files for a given session. If unspecified,         the output is stored in the video folder for the session, which is the default behavior of Facemap.         Returns a string of the absolute path of the output directory.</p> Source code in <code>element_facemap/facial_behavior_estimation.py</code> <pre><code>def activate(\n    facemap_schema_name, *, create_schema=True, create_tables=True, linking_module=None\n):\n    \"\"\"Activate schema.\n\n    Args:\n        facemap_schema_name (str): Schema name on the database server to activate the\n            `facemap` element\n        create_schema (bool): When True (default), create schema in the database if it\n            does not yet exist.\n        create_tables (bool): When True (default), create tables in the database if\n            they do not yet exist.\n        linking_module (str): A module name or a module containing the required\n            dependencies to activate the `facial_behavior_estimation` module:\n\n    Dependencies:\n    Upstream tables:\n        + Session: A parent table to VideoRecording, identifying a recording session\n        + Equipment: A parent table to VideoRecording, identifying video recording equipment\n    Functions:\n        + get_facemap_root_data_dir() -&gt; list\n            Retrieves the root data directory(s) with face recordings for all\n            subject/sessions. Returns a string for the full path to the root data directory.\n        + get_facemap_processed_data_dir(session_key: dict) -&gt; str\n            Optional function to retrieve the desired output directory\n            for Facemap files for a given session. If unspecified,\n            the output is stored in the video folder for the session, which is the default behavior of Facemap.\n            Returns a string of the absolute path of the output directory.\n    \"\"\"\n    if isinstance(linking_module, str):\n        linking_module = importlib.import_module(linking_module)\n    assert inspect.ismodule(\n        linking_module\n    ), \"The argument 'dependency' must be a module's name or a module\"\n    assert hasattr(\n        linking_module, \"get_facemap_root_data_dir\"\n    ), \"The linking module must specify a lookup function for a root data directory\"\n\n    global _linking_module\n    _linking_module = linking_module\n\n    # activate\n    schema.activate(\n        facemap_schema_name,\n        create_schema=create_schema,\n        create_tables=create_tables,\n        add_objects=_linking_module.__dict__,\n    )\n</code></pre>"}, {"location": "api/element_facemap/facial_behavior_estimation/#element_facemap.facial_behavior_estimation.get_facemap_root_data_dir", "title": "<code>get_facemap_root_data_dir()</code>", "text": "<p>Pull the relevant function from the parent namespace to specify the root data directory(s).</p> <p>It is recommended that all paths in DataJoint Elements are stored as relative paths, with respect to some user-configured \"root\" directory. The root(s) may vary between data modalities and user machines.</p> <p>Returns:</p> Name Type Description <code>paths</code> <code>list</code> <p>list of path(s) to root data directory(s) for Facemap</p> Source code in <code>element_facemap/facial_behavior_estimation.py</code> <pre><code>def get_facemap_root_data_dir():\n    \"\"\"Pull the relevant function from the parent namespace to specify the root data directory(s).\n\n    It is recommended that all paths in DataJoint Elements are stored as relative\n    paths, with respect to some user-configured \"root\" directory. The\n    root(s) may vary between data modalities and user machines.\n\n    Returns:\n        paths (list): list of path(s) to root data directory(s) for Facemap\n    \"\"\"\n    root_directories = _linking_module.get_facemap_root_data_dir()\n    if isinstance(root_directories, (str, Path)):\n        root_directories = [root_directories]\n\n    if hasattr(_linking_module, \"get_facemap_processed_data_dir\"):\n        root_directories.append(_linking_module.get_facemap_processed_data_dir())\n\n    return root_directories\n</code></pre>"}, {"location": "api/element_facemap/facial_behavior_estimation/#element_facemap.facial_behavior_estimation.get_facemap_processed_data_dir", "title": "<code>get_facemap_processed_data_dir()</code>", "text": "<p>Facemap output directory</p> <p>If specified by the user, this function provides Facemap with an output directory for processed files. If unspecified, the output is stored in the video directory for the session, which is the default behavior of Facemap.</p> <p>Returns:</p> Name Type Description <code>path</code> <code>str</code> <p>path to Facemap output directory</p> Source code in <code>element_facemap/facial_behavior_estimation.py</code> <pre><code>def get_facemap_processed_data_dir() -&gt; str:\n    \"\"\"Facemap output directory\n\n    If specified by the user, this function provides Facemap with an output\n    directory for processed files. If unspecified, the output is stored in the video directory for the session, which is the default behavior of Facemap.\n\n    Returns:\n        path (str): path to Facemap output directory\n    \"\"\"\n    if hasattr(_linking_module, \"get_facemap_processed_data_dir\"):\n        return _linking_module.get_facemap_processed_data_dir()\n    else:\n        return get_facemap_root_data_dir()[0]\n</code></pre>"}, {"location": "api/element_facemap/facial_behavior_estimation/#element_facemap.facial_behavior_estimation.get_facemap_video_files", "title": "<code>get_facemap_video_files(video_key)</code>", "text": "<p>Retrieve the list of video recording files.</p> <p>Parameters:</p> Name Type Description Default <code>video_key</code> <code>dict</code> <p>Primary key of an entry in the VideoRecording table.</p> required <p>Returns:</p> Type Description <code>List[Path]</code> <p>List of absolute POSIX paths of the video files.</p> Source code in <code>element_facemap/facial_behavior_estimation.py</code> <pre><code>def get_facemap_video_files(video_key: dict) -&gt; List[Path]:\n    \"\"\"Retrieve the list of video recording files.\n\n    Args:\n        video_key: Primary key of an entry in the VideoRecording table.\n\n    Returns:\n        List of absolute POSIX paths of the video files.\n    \"\"\"\n    return _linking_module.get_facemap_video_files(video_key)\n</code></pre>"}, {"location": "api/element_facemap/facial_behavior_estimation/#element_facemap.facial_behavior_estimation.VideoRecording", "title": "<code>VideoRecording</code>", "text": "<p>               Bases: <code>Manual</code></p> <p>Video recorded in an experiment session for Facemap analysis.</p> <p>Attributes:</p> Name Type Description <code>Session</code> <code>foreign key</code> <p>Primary key from Session.</p> <code>recording_id</code> <code>int</code> <p>Recording ID.</p> <code>Device</code> <code>foreign key</code> <p>Primary key from Device.</p> Source code in <code>element_facemap/facial_behavior_estimation.py</code> <pre><code>@schema\nclass VideoRecording(dj.Manual):\n    \"\"\"Video recorded in an experiment session for Facemap analysis.\n\n    Attributes:\n        Session (foreign key): Primary key from Session.\n        recording_id (int): Recording ID.\n        Device (foreign key): Primary key from Device.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; Session\n    recording_id                : int\n    ---\n    -&gt; Device\n    \"\"\"\n\n    # One VideoRecording can be saved in multiple files\n    class File(dj.Part):\n        \"\"\"Relative path of video file with respect to facemap_root_data_dir directory.\n\n        Attributes:\n            master (foreign key): Primary key from VideoRecording.\n            file_id (smallint): File ID.\n            file_path (str): Filepath of video, relative to root directory.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        file_id     : smallint\n        ---\n        file_path   : varchar(255)  # filepath of video, relative to root directory\n        \"\"\"\n</code></pre>"}, {"location": "api/element_facemap/facial_behavior_estimation/#element_facemap.facial_behavior_estimation.VideoRecording.File", "title": "<code>File</code>", "text": "<p>               Bases: <code>Part</code></p> <p>Relative path of video file with respect to facemap_root_data_dir directory.</p> <p>Attributes:</p> Name Type Description <code>master</code> <code>foreign key</code> <p>Primary key from VideoRecording.</p> <code>file_id</code> <code>smallint</code> <p>File ID.</p> <code>file_path</code> <code>str</code> <p>Filepath of video, relative to root directory.</p> Source code in <code>element_facemap/facial_behavior_estimation.py</code> <pre><code>class File(dj.Part):\n    \"\"\"Relative path of video file with respect to facemap_root_data_dir directory.\n\n    Attributes:\n        master (foreign key): Primary key from VideoRecording.\n        file_id (smallint): File ID.\n        file_path (str): Filepath of video, relative to root directory.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    file_id     : smallint\n    ---\n    file_path   : varchar(255)  # filepath of video, relative to root directory\n    \"\"\"\n</code></pre>"}, {"location": "api/element_facemap/facial_behavior_estimation/#element_facemap.facial_behavior_estimation.RecordingInfo", "title": "<code>RecordingInfo</code>", "text": "<p>               Bases: <code>Imported</code></p> <p>Information extracted from video file.</p> <p>Attributes:</p> Name Type Description <code>VideoRecording</code> <code>foreign key</code> <p>Primary key for VideoRecording table.</p> <code>px_height</code> <code>int</code> <p>Height in pixels.</p> <code>px_width</code> <code>int</code> <p>Width in pixels.</p> <code>nframes</code> <code>int</code> <p>Number of frames.</p> <code>fps</code> <code>int</code> <p>Frames per second in Hz.</p> <code>recording_duration</code> <code>float</code> <p>Video duration in seconds.</p> <code>recording_time</code> <code>datetime</code> <p>Time at the beginning of the recording.</p> Source code in <code>element_facemap/facial_behavior_estimation.py</code> <pre><code>@schema\nclass RecordingInfo(dj.Imported):\n    \"\"\"Information extracted from video file.\n\n    Attributes:\n        VideoRecording (foreign key): Primary key for VideoRecording table.\n        px_height (int): Height in pixels.\n        px_width (int): Width in pixels.\n        nframes (int): Number of frames.\n        fps (int): Frames per second in Hz.\n        recording_duration (float): Video duration in seconds.\n        recording_time (datetime, optional): Time at the beginning of the recording.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; VideoRecording\n    ---\n    px_height             : int       # height in pixels\n    px_width              : int       # width in pixels\n    nframes               : int       # number of frames\n    fps                   : int       # frames per second in Hz\n    recording_duration    : float     # video duration in seconds\n    recording_time = NULL : datetime  # time at the beginning of the recording\n    \"\"\"\n\n    @property\n    def key_source(self):\n        \"\"\"Limits the population of RecordingInfo to video recordings that have file paths ingested.\"\"\"\n        return VideoRecording &amp; VideoRecording.File\n\n    def make(self, key):\n        \"\"\"Populates the RecordingInfo table.\"\"\"\n\n        file_paths = (VideoRecording.File &amp; key).fetch(\"file_path\")\n\n        nframes = 0\n        px_height, px_width, fps = None, None, None\n\n        for file_path in file_paths:\n            file_path = (\n                find_full_path(get_facemap_root_data_dir(), file_path)\n            ).as_posix()\n\n            cap = cv2.VideoCapture(file_path)\n            info = (\n                int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),\n                int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n                int(cap.get(cv2.CAP_PROP_FPS)),\n            )\n            if px_height is not None:\n                assert (px_height, px_width, fps) == info\n            px_height, px_width, fps = info\n            nframes += int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n            cap.release()\n\n        self.insert1(\n            {\n                **key,\n                \"px_height\": px_height,\n                \"px_width\": px_width,\n                \"nframes\": nframes,\n                \"fps\": fps,  # usually user-defined and wrong\n                \"recording_duration\": nframes / fps,  # see caution above\n            }\n        )\n</code></pre>"}, {"location": "api/element_facemap/facial_behavior_estimation/#element_facemap.facial_behavior_estimation.RecordingInfo.key_source", "title": "<code>key_source</code>  <code>property</code>", "text": "<p>Limits the population of RecordingInfo to video recordings that have file paths ingested.</p>"}, {"location": "api/element_facemap/facial_behavior_estimation/#element_facemap.facial_behavior_estimation.RecordingInfo.make", "title": "<code>make(key)</code>", "text": "<p>Populates the RecordingInfo table.</p> Source code in <code>element_facemap/facial_behavior_estimation.py</code> <pre><code>def make(self, key):\n    \"\"\"Populates the RecordingInfo table.\"\"\"\n\n    file_paths = (VideoRecording.File &amp; key).fetch(\"file_path\")\n\n    nframes = 0\n    px_height, px_width, fps = None, None, None\n\n    for file_path in file_paths:\n        file_path = (\n            find_full_path(get_facemap_root_data_dir(), file_path)\n        ).as_posix()\n\n        cap = cv2.VideoCapture(file_path)\n        info = (\n            int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),\n            int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n            int(cap.get(cv2.CAP_PROP_FPS)),\n        )\n        if px_height is not None:\n            assert (px_height, px_width, fps) == info\n        px_height, px_width, fps = info\n        nframes += int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        cap.release()\n\n    self.insert1(\n        {\n            **key,\n            \"px_height\": px_height,\n            \"px_width\": px_width,\n            \"nframes\": nframes,\n            \"fps\": fps,  # usually user-defined and wrong\n            \"recording_duration\": nframes / fps,  # see caution above\n        }\n    )\n</code></pre>"}, {"location": "api/element_facemap/facial_behavior_estimation/#element_facemap.facial_behavior_estimation.FacemapTask", "title": "<code>FacemapTask</code>", "text": "<p>               Bases: <code>Manual</code></p> <p>Staging table for pairing of recording and Facemap parameters before processing.</p> <p>Attributes:</p> Name Type Description <code>VideoRecording</code> <code>foreign key</code> <p>Primary key for VideoRecording table.</p> <code>facemap_task_id</code> <code>smallint</code> <p>Facemap task ID</p> <code>facemap_output_dir</code> <code>str</code> <p>output dir storing the results of Facemap analysis.</p> <code>task_mode</code> <code>enum</code> <p>Default load. Load or trigger analysis.</p> <code>facemap_params</code> <code>longblob</code> <p>content of facemap's _proc.npy as dict.</p> <code>do_mot_svd</code> <code>bool</code> <p>Default True. Do motion singular value decomposition.</p> <code>do_mov_svd</code> <code>bool</code> <p>Default False. Do movie singular value decomposition.</p> <code>task_description</code> <code>str</code> <p>Task description.</p> Source code in <code>element_facemap/facial_behavior_estimation.py</code> <pre><code>@schema\nclass FacemapTask(dj.Manual):\n    \"\"\"Staging table for pairing of recording and Facemap parameters before processing.\n\n    Attributes:\n        VideoRecording (foreign key): Primary key for VideoRecording table.\n        facemap_task_id (smallint): Facemap task ID\n        facemap_output_dir (str, optional): output dir storing the results\n            of Facemap analysis.\n        task_mode (enum): Default load. Load or trigger analysis.\n        facemap_params (longblob): content of facemap's _proc.npy as dict.\n        do_mot_svd (bool): Default True. Do motion singular value decomposition.\n        do_mov_svd (bool): Default False. Do movie singular value decomposition.\n        task_description (str, optional): Task description.\n    \"\"\"\n\n    definition = \"\"\"\n    # Staging table for pairing of recording and Facemap parameters before processing.\n    -&gt; VideoRecording\n    facemap_task_id             : smallint\n    ---\n    facemap_output_dir=''       : varchar(255)  # output directory - storing the results of Facemap analysis\n    task_mode='load'            : enum('load', 'trigger')\n    facemap_params              : longblob  # content of facemap's _proc.npy as dict\n    do_mot_svd=1                : bool\n    do_mov_svd=0                : bool\n    task_description=''         : varchar(128)\n    \"\"\"\n\n    def infer_output_dir(self, key, relative=True, mkdir=True):\n        video_file = (VideoRecording.File &amp; key).fetch(\"file_path\", limit=1)[0]\n        video_dir = find_full_path(get_facemap_root_data_dir(), video_file).parent\n        root_dir = find_root_directory(get_facemap_root_data_dir(), video_dir)\n\n        paramset_key = (FacemapTask &amp; key).fetch1(\"facemap_task_id\")\n        processed_dir = Path(get_facemap_processed_data_dir())\n        output_dir = (\n            processed_dir / video_dir.relative_to(root_dir) / f\"facemap_{paramset_key}\"\n        )\n\n        if mkdir:\n            output_dir.mkdir(parents=True, exist_ok=True)\n\n        return output_dir.relative_to(processed_dir) if relative else output_dir\n</code></pre>"}, {"location": "api/element_facemap/facial_behavior_estimation/#element_facemap.facial_behavior_estimation.FacemapProcessing", "title": "<code>FacemapProcessing</code>", "text": "<p>               Bases: <code>Computed</code></p> <p>Automated table to run Facemap with inputs from FacemapTask.</p> <p>Attributes:</p> Name Type Description <code>FacemapTask</code> <code>foreign key</code> <p>Primary key from FacemapTask.</p> <code>processing_time</code> <code>datetime</code> <p>Time of generation of the facemap results.</p> <code>package_version</code> <code>str</code> <p>Facemap package version.</p> Source code in <code>element_facemap/facial_behavior_estimation.py</code> <pre><code>@schema\nclass FacemapProcessing(dj.Computed):\n    \"\"\"Automated table to run Facemap with inputs from FacemapTask.\n\n    Attributes:\n        FacemapTask (foreign key): Primary key from FacemapTask.\n        processing_time (datetime): Time of generation of the facemap results.\n        package_version (str, optional): Facemap package version.\n    \"\"\"\n\n    definition = \"\"\"\n    # Processing Procedure\n    -&gt; FacemapTask\n    ---\n    processing_time     : datetime  # time of generation of the facemap results\n    package_version=''  : varchar(16)\n    \"\"\"\n\n    # Process only the VideoRecordings that have their Info inserted.\n    @property\n    def key_source(self):\n        \"\"\"Limits the population of FacemapProcessing to those that have VideoRecording.File defined.\"\"\"\n        return FacemapTask &amp; VideoRecording.File\n\n    def make(self, key):\n        \"\"\"Runs Facemap\"\"\"\n\n        task_mode = (FacemapTask &amp; key).fetch1(\"task_mode\")\n\n        output_dir = (FacemapTask &amp; key).fetch1(\"facemap_output_dir\")\n        if not output_dir:\n            output_dir = FacemapTask().infer_output_dir(key, relative=True, mkdir=True)\n            # update processing_output_dir\n            FacemapTask.update1({**key, \"facemap_output_dir\": output_dir.as_posix()})\n\n        if task_mode == \"trigger\":\n            from facemap.process import run as facemap_run\n\n            params = (FacemapTask &amp; key).fetch1(\"facemap_params\")\n\n            video_files = (FacemapTask * VideoRecording.File &amp; key).fetch(\"file_path\")\n            video_files = [\n                [\n                    find_full_path(get_facemap_root_data_dir(), video_file).as_posix()\n                    for video_file in video_files\n                ]\n            ]\n\n            output_dir = find_full_path(get_facemap_root_data_dir(), output_dir)\n            facemap_run(\n                video_files,\n                sbin=params[\"sbin\"],\n                proc=params,\n                savepath=output_dir.as_posix(),\n                motSVD=params[\"motSVD\"],\n                movSVD=params[\"movSVD\"],\n            )\n\n        _, creation_time = get_loader_result(key, FacemapTask)\n        key = {**key, \"processing_time\": creation_time}\n\n        self.insert1(key)\n</code></pre>"}, {"location": "api/element_facemap/facial_behavior_estimation/#element_facemap.facial_behavior_estimation.FacemapProcessing.key_source", "title": "<code>key_source</code>  <code>property</code>", "text": "<p>Limits the population of FacemapProcessing to those that have VideoRecording.File defined.</p>"}, {"location": "api/element_facemap/facial_behavior_estimation/#element_facemap.facial_behavior_estimation.FacemapProcessing.make", "title": "<code>make(key)</code>", "text": "<p>Runs Facemap</p> Source code in <code>element_facemap/facial_behavior_estimation.py</code> <pre><code>def make(self, key):\n    \"\"\"Runs Facemap\"\"\"\n\n    task_mode = (FacemapTask &amp; key).fetch1(\"task_mode\")\n\n    output_dir = (FacemapTask &amp; key).fetch1(\"facemap_output_dir\")\n    if not output_dir:\n        output_dir = FacemapTask().infer_output_dir(key, relative=True, mkdir=True)\n        # update processing_output_dir\n        FacemapTask.update1({**key, \"facemap_output_dir\": output_dir.as_posix()})\n\n    if task_mode == \"trigger\":\n        from facemap.process import run as facemap_run\n\n        params = (FacemapTask &amp; key).fetch1(\"facemap_params\")\n\n        video_files = (FacemapTask * VideoRecording.File &amp; key).fetch(\"file_path\")\n        video_files = [\n            [\n                find_full_path(get_facemap_root_data_dir(), video_file).as_posix()\n                for video_file in video_files\n            ]\n        ]\n\n        output_dir = find_full_path(get_facemap_root_data_dir(), output_dir)\n        facemap_run(\n            video_files,\n            sbin=params[\"sbin\"],\n            proc=params,\n            savepath=output_dir.as_posix(),\n            motSVD=params[\"motSVD\"],\n            movSVD=params[\"movSVD\"],\n        )\n\n    _, creation_time = get_loader_result(key, FacemapTask)\n    key = {**key, \"processing_time\": creation_time}\n\n    self.insert1(key)\n</code></pre>"}, {"location": "api/element_facemap/facial_behavior_estimation/#element_facemap.facial_behavior_estimation.FacialSignal", "title": "<code>FacialSignal</code>", "text": "<p>               Bases: <code>Imported</code></p> <p>Results of the Facemap analysis.</p> <p>Attributes:</p> Name Type Description <code>FacemapProcessing</code> <code>foreign key) </code> <p>Primary key for FacemapProcessing table.</p> Source code in <code>element_facemap/facial_behavior_estimation.py</code> <pre><code>@schema\nclass FacialSignal(dj.Imported):\n    \"\"\"Results of the Facemap analysis.\n\n    Attributes:\n        FacemapProcessing (foreign key) : Primary key for FacemapProcessing table.\n    \"\"\"\n\n    definition = \"\"\"# Facemap results\n    -&gt; FacemapProcessing\n    \"\"\"\n\n    class Region(dj.Part):\n        \"\"\"Region's properties.\n\n        Attributes:\n            master (foreign key): Primary key of the FacialSignal table.\n            roi_no (int): Region number.\n            roi_name (str, optional): User-friendly name of the roi.\n            xrange (longblob): 1d np.array - x pixel indices.\n            yrange (longblob): 1d np.array - y pixel indices.\n            xrange_bin (longblob): 1d np.array - binned x pixel indices.\n            yrange_bin (longblob): 1d np.array - binned y pixel indices.\n            motion (longblob): 1d np.array - absolute motion energies (nframes).\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        roi_no        : int         # Region number\n        ---\n        roi_name=''   : varchar(16) # user-friendly name of the roi\n        xrange        : longblob    # 1d np.array - x pixel indices\n        yrange        : longblob    # 1d np.array - y pixel indices\n        xrange_bin    : longblob    # 1d np.array - binned x pixel indices\n        yrange_bin    : longblob    # 1d np.array - binned y pixel indices\n        motion        : longblob    # 1d np.array - absolute motion energies (nframes)\n        \"\"\"\n\n    class MotionSVD(dj.Part):\n        \"\"\"Components of the SVD from motion video.\n\n        Attributes:\n            master.Region (foreign key): Primary key from FacialSignal.Region.\n            pc_no (int): Principle component (PC) number.\n            singular_value (float, optional): singular value corresponding to the PC.\n            motmask (longblob): PC (y, x).\n            projection (longblob): projections onto the principle component (nframes).\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master.Region\n        pc_no               : int         # principle component (PC) number\n        ---\n        singular_value=null : float       # singular value corresponding to the PC\n        motmask             : longblob    # PC (y, x)\n        projection          : longblob    # projections onto the principle component (nframes)\n        \"\"\"\n\n    class MovieSVD(dj.Part):\n        \"\"\"Components of the SVD from movie video.\n\n        Attributes:\n            master.Region (foreign key): Primary key of the FacialSignal.Region table.\n            pc_no (int): principle component (PC) number.\n            singular_value (float, optional): Singular value corresponding to the PC.\n            movmask (longblob): PC (y, x)\n            projection (longblob): Projections onto the principle component (nframes).\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master.Region\n        pc_no               : int         # principle component (PC) number\n        ---\n        singular_value=null : float       # singular value corresponding to the PC\n        movmask             : longblob    # PC (y, x)\n        projection          : longblob    # projections onto the principle component (nframes)\n        \"\"\"\n\n    class Summary(dj.Part):\n        \"\"\"Average frames for movie and motion videos.\n\n        Attributes:\n            master (foreign key): Primary key from FacialSignal.\n            sbin (int): Spatial bin size.\n            avgframe (longblob): 2d np.array - average binned frame.\n            avgmotion (longblob): 2d nd.array - average binned motion frame.\n        \"\"\"\n\n        definition = \"\"\"\n        -&gt; master\n        ---\n        sbin          : int         # spatial bin size\n        avgframe      : longblob    # 2d np.array - average binned frame\n        avgmotion     : longblob    # 2d nd.array - average binned motion frame\n        \"\"\"\n\n    def make(self, key):\n        \"\"\"Populates the FacialSignal table by transferring the results from default\n        Facemap outputs to the database.\"\"\"\n\n        dataset, _ = get_loader_result(key, FacemapTask)\n        # Only motion SVD region type is supported.\n        dataset[\"rois\"] = [x for x in dataset[\"rois\"] if x[\"rtype\"] == \"motion SVD\"]\n\n        self.insert1(key)\n\n        self.Region.insert(\n            [\n                dict(\n                    key,\n                    roi_no=i,\n                    xrange=dataset[\"rois\"][i][\"xrange\"],\n                    yrange=dataset[\"rois\"][i][\"yrange\"],\n                    xrange_bin=(\n                        dataset[\"rois\"][i][\"xrange_bin\"]\n                        if \"xrange_bin\" in dataset[\"rois\"][i]\n                        else None\n                    ),\n                    yrange_bin=(\n                        dataset[\"rois\"][i][\"yrange_bin\"]\n                        if \"yrange_bin\" in dataset[\"rois\"][i]\n                        else None\n                    ),\n                    motion=dataset[\"motion\"][i + 1],\n                )\n                for i in range(len(dataset[\"rois\"]))\n                if dataset[\"rois\"][i][\"rtype\"] == \"motion SVD\"\n            ]\n        )\n\n        # MotionSVD\n        if any(np.any(x) for x in dataset.get(\"motSVD\", [False])):\n            entry = [\n                dict(\n                    key,\n                    roi_no=roi_no,\n                    pc_no=i,\n                    singular_value=(\n                        dataset[\"motSv\"][roi_no][i] if \"motSv\" in dataset else None\n                    ),\n                    motmask=dataset[\"motMask_reshape\"][roi_no + 1][:, :, i],\n                    projection=dataset[\"motSVD\"][roi_no + 1][i],\n                )\n                for roi_no in range(len(dataset[\"rois\"]))\n                for i in range(dataset[\"motSVD\"][roi_no + 1].shape[1])\n            ]\n            self.MotionSVD.insert(entry)\n\n        # MovieSVD\n        if any(np.any(x) for x in dataset.get(\"movSVD\", [False])):\n            entry = [\n                dict(\n                    key,\n                    roi_no=roi_no,\n                    pc_no=i,\n                    singular_value=(\n                        dataset[\"movSv\"][roi_no][i] if \"movSv\" in dataset else None\n                    ),\n                    movmask=dataset[\"movMask_reshape\"][roi_no + 1][:, :, i],\n                    projection=dataset[\"movSVD\"][roi_no + 1][i],\n                )\n                for roi_no in range(len(dataset[\"rois\"]))\n                for i in range(dataset[\"movSVD\"][roi_no + 1].shape[1])\n            ]\n            self.MovieSVD.insert(entry)\n\n        # Summary\n        self.Summary.insert1(\n            dict(\n                key,\n                sbin=dataset[\"sbin\"],\n                avgframe=dataset[\"avgframe\"][0],\n                avgmotion=dataset[\"avgmotion\"][0],\n            )\n        )\n</code></pre>"}, {"location": "api/element_facemap/facial_behavior_estimation/#element_facemap.facial_behavior_estimation.FacialSignal.Region", "title": "<code>Region</code>", "text": "<p>               Bases: <code>Part</code></p> <p>Region's properties.</p> <p>Attributes:</p> Name Type Description <code>master</code> <code>foreign key</code> <p>Primary key of the FacialSignal table.</p> <code>roi_no</code> <code>int</code> <p>Region number.</p> <code>roi_name</code> <code>str</code> <p>User-friendly name of the roi.</p> <code>xrange</code> <code>longblob</code> <p>1d np.array - x pixel indices.</p> <code>yrange</code> <code>longblob</code> <p>1d np.array - y pixel indices.</p> <code>xrange_bin</code> <code>longblob</code> <p>1d np.array - binned x pixel indices.</p> <code>yrange_bin</code> <code>longblob</code> <p>1d np.array - binned y pixel indices.</p> <code>motion</code> <code>longblob</code> <p>1d np.array - absolute motion energies (nframes).</p> Source code in <code>element_facemap/facial_behavior_estimation.py</code> <pre><code>class Region(dj.Part):\n    \"\"\"Region's properties.\n\n    Attributes:\n        master (foreign key): Primary key of the FacialSignal table.\n        roi_no (int): Region number.\n        roi_name (str, optional): User-friendly name of the roi.\n        xrange (longblob): 1d np.array - x pixel indices.\n        yrange (longblob): 1d np.array - y pixel indices.\n        xrange_bin (longblob): 1d np.array - binned x pixel indices.\n        yrange_bin (longblob): 1d np.array - binned y pixel indices.\n        motion (longblob): 1d np.array - absolute motion energies (nframes).\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    roi_no        : int         # Region number\n    ---\n    roi_name=''   : varchar(16) # user-friendly name of the roi\n    xrange        : longblob    # 1d np.array - x pixel indices\n    yrange        : longblob    # 1d np.array - y pixel indices\n    xrange_bin    : longblob    # 1d np.array - binned x pixel indices\n    yrange_bin    : longblob    # 1d np.array - binned y pixel indices\n    motion        : longblob    # 1d np.array - absolute motion energies (nframes)\n    \"\"\"\n</code></pre>"}, {"location": "api/element_facemap/facial_behavior_estimation/#element_facemap.facial_behavior_estimation.FacialSignal.MotionSVD", "title": "<code>MotionSVD</code>", "text": "<p>               Bases: <code>Part</code></p> <p>Components of the SVD from motion video.</p> <p>Attributes:</p> Name Type Description <code>master.Region</code> <code>foreign key</code> <p>Primary key from FacialSignal.Region.</p> <code>pc_no</code> <code>int</code> <p>Principle component (PC) number.</p> <code>singular_value</code> <code>float</code> <p>singular value corresponding to the PC.</p> <code>motmask</code> <code>longblob</code> <p>PC (y, x).</p> <code>projection</code> <code>longblob</code> <p>projections onto the principle component (nframes).</p> Source code in <code>element_facemap/facial_behavior_estimation.py</code> <pre><code>class MotionSVD(dj.Part):\n    \"\"\"Components of the SVD from motion video.\n\n    Attributes:\n        master.Region (foreign key): Primary key from FacialSignal.Region.\n        pc_no (int): Principle component (PC) number.\n        singular_value (float, optional): singular value corresponding to the PC.\n        motmask (longblob): PC (y, x).\n        projection (longblob): projections onto the principle component (nframes).\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master.Region\n    pc_no               : int         # principle component (PC) number\n    ---\n    singular_value=null : float       # singular value corresponding to the PC\n    motmask             : longblob    # PC (y, x)\n    projection          : longblob    # projections onto the principle component (nframes)\n    \"\"\"\n</code></pre>"}, {"location": "api/element_facemap/facial_behavior_estimation/#element_facemap.facial_behavior_estimation.FacialSignal.MovieSVD", "title": "<code>MovieSVD</code>", "text": "<p>               Bases: <code>Part</code></p> <p>Components of the SVD from movie video.</p> <p>Attributes:</p> Name Type Description <code>master.Region</code> <code>foreign key</code> <p>Primary key of the FacialSignal.Region table.</p> <code>pc_no</code> <code>int</code> <p>principle component (PC) number.</p> <code>singular_value</code> <code>float</code> <p>Singular value corresponding to the PC.</p> <code>movmask</code> <code>longblob</code> <p>PC (y, x)</p> <code>projection</code> <code>longblob</code> <p>Projections onto the principle component (nframes).</p> Source code in <code>element_facemap/facial_behavior_estimation.py</code> <pre><code>class MovieSVD(dj.Part):\n    \"\"\"Components of the SVD from movie video.\n\n    Attributes:\n        master.Region (foreign key): Primary key of the FacialSignal.Region table.\n        pc_no (int): principle component (PC) number.\n        singular_value (float, optional): Singular value corresponding to the PC.\n        movmask (longblob): PC (y, x)\n        projection (longblob): Projections onto the principle component (nframes).\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master.Region\n    pc_no               : int         # principle component (PC) number\n    ---\n    singular_value=null : float       # singular value corresponding to the PC\n    movmask             : longblob    # PC (y, x)\n    projection          : longblob    # projections onto the principle component (nframes)\n    \"\"\"\n</code></pre>"}, {"location": "api/element_facemap/facial_behavior_estimation/#element_facemap.facial_behavior_estimation.FacialSignal.Summary", "title": "<code>Summary</code>", "text": "<p>               Bases: <code>Part</code></p> <p>Average frames for movie and motion videos.</p> <p>Attributes:</p> Name Type Description <code>master</code> <code>foreign key</code> <p>Primary key from FacialSignal.</p> <code>sbin</code> <code>int</code> <p>Spatial bin size.</p> <code>avgframe</code> <code>longblob</code> <p>2d np.array - average binned frame.</p> <code>avgmotion</code> <code>longblob</code> <p>2d nd.array - average binned motion frame.</p> Source code in <code>element_facemap/facial_behavior_estimation.py</code> <pre><code>class Summary(dj.Part):\n    \"\"\"Average frames for movie and motion videos.\n\n    Attributes:\n        master (foreign key): Primary key from FacialSignal.\n        sbin (int): Spatial bin size.\n        avgframe (longblob): 2d np.array - average binned frame.\n        avgmotion (longblob): 2d nd.array - average binned motion frame.\n    \"\"\"\n\n    definition = \"\"\"\n    -&gt; master\n    ---\n    sbin          : int         # spatial bin size\n    avgframe      : longblob    # 2d np.array - average binned frame\n    avgmotion     : longblob    # 2d nd.array - average binned motion frame\n    \"\"\"\n</code></pre>"}, {"location": "api/element_facemap/facial_behavior_estimation/#element_facemap.facial_behavior_estimation.FacialSignal.make", "title": "<code>make(key)</code>", "text": "<p>Populates the FacialSignal table by transferring the results from default Facemap outputs to the database.</p> Source code in <code>element_facemap/facial_behavior_estimation.py</code> <pre><code>def make(self, key):\n    \"\"\"Populates the FacialSignal table by transferring the results from default\n    Facemap outputs to the database.\"\"\"\n\n    dataset, _ = get_loader_result(key, FacemapTask)\n    # Only motion SVD region type is supported.\n    dataset[\"rois\"] = [x for x in dataset[\"rois\"] if x[\"rtype\"] == \"motion SVD\"]\n\n    self.insert1(key)\n\n    self.Region.insert(\n        [\n            dict(\n                key,\n                roi_no=i,\n                xrange=dataset[\"rois\"][i][\"xrange\"],\n                yrange=dataset[\"rois\"][i][\"yrange\"],\n                xrange_bin=(\n                    dataset[\"rois\"][i][\"xrange_bin\"]\n                    if \"xrange_bin\" in dataset[\"rois\"][i]\n                    else None\n                ),\n                yrange_bin=(\n                    dataset[\"rois\"][i][\"yrange_bin\"]\n                    if \"yrange_bin\" in dataset[\"rois\"][i]\n                    else None\n                ),\n                motion=dataset[\"motion\"][i + 1],\n            )\n            for i in range(len(dataset[\"rois\"]))\n            if dataset[\"rois\"][i][\"rtype\"] == \"motion SVD\"\n        ]\n    )\n\n    # MotionSVD\n    if any(np.any(x) for x in dataset.get(\"motSVD\", [False])):\n        entry = [\n            dict(\n                key,\n                roi_no=roi_no,\n                pc_no=i,\n                singular_value=(\n                    dataset[\"motSv\"][roi_no][i] if \"motSv\" in dataset else None\n                ),\n                motmask=dataset[\"motMask_reshape\"][roi_no + 1][:, :, i],\n                projection=dataset[\"motSVD\"][roi_no + 1][i],\n            )\n            for roi_no in range(len(dataset[\"rois\"]))\n            for i in range(dataset[\"motSVD\"][roi_no + 1].shape[1])\n        ]\n        self.MotionSVD.insert(entry)\n\n    # MovieSVD\n    if any(np.any(x) for x in dataset.get(\"movSVD\", [False])):\n        entry = [\n            dict(\n                key,\n                roi_no=roi_no,\n                pc_no=i,\n                singular_value=(\n                    dataset[\"movSv\"][roi_no][i] if \"movSv\" in dataset else None\n                ),\n                movmask=dataset[\"movMask_reshape\"][roi_no + 1][:, :, i],\n                projection=dataset[\"movSVD\"][roi_no + 1][i],\n            )\n            for roi_no in range(len(dataset[\"rois\"]))\n            for i in range(dataset[\"movSVD\"][roi_no + 1].shape[1])\n        ]\n        self.MovieSVD.insert(entry)\n\n    # Summary\n    self.Summary.insert1(\n        dict(\n            key,\n            sbin=dataset[\"sbin\"],\n            avgframe=dataset[\"avgframe\"][0],\n            avgmotion=dataset[\"avgmotion\"][0],\n        )\n    )\n</code></pre>"}, {"location": "api/element_facemap/facial_behavior_estimation/#element_facemap.facial_behavior_estimation.get_loader_result", "title": "<code>get_loader_result(key, table)</code>", "text": "<p>Retrieve the facemap analysis results.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>A primary key for an entry in the provided table.</p> required <code>table</code> <code>Table</code> <p>DataJoint user table from which loaded results are retrieved (i.e. FacemapTask).</p> required <p>Returns:</p> Name Type Description <code>loaded_dataset</code> <code>array</code> <p>The results of the facemap analysis.</p> <code>creation_time</code> <code>datetime</code> <p>Date and time that the results files were created.</p> Source code in <code>element_facemap/facial_behavior_estimation.py</code> <pre><code>def get_loader_result(\n    key: dict, table: dj.user_tables.TableMeta\n) -&gt; Tuple[np.array, datetime]:\n    \"\"\"Retrieve the facemap analysis results.\n\n    Args:\n        key (dict): A primary key for an entry in the provided table.\n        table (dj.Table): DataJoint user table from which loaded results are retrieved (i.e. FacemapTask).\n\n    Returns:\n        loaded_dataset (np.array): The results of the facemap analysis.\n        creation_time (datetime): Date and time that the results files were created.\n    \"\"\"\n    output_dir = (table &amp; key).fetch1(\"facemap_output_dir\")\n\n    output_path = find_full_path(get_facemap_root_data_dir(), output_dir)\n    output_file = glob(output_path.as_posix() + \"/*_proc.npy\")[0]\n\n    loaded_dataset = np.load(output_file, allow_pickle=True).item()\n    creation_time = datetime.fromtimestamp(Path(output_file).stat().st_ctime)\n\n    return loaded_dataset, creation_time\n</code></pre>"}, {"location": "tutorials/", "title": "Tutorials", "text": "<ul> <li>Element Facemap includes an interactive tutorial on GitHub Codespaces, which is configured for users to run the pipeline.</li> </ul> <ul> <li>DataJoint Elements are modular and can be connected into a complete pipeline.  In the interactive tutorial is an example Jupyter notebook that combines four DataJoint Elements - Lab, Animal, Session, and Facemap.  The notebook describes the pipeline and provides instructions for running the pipeline.  For convenience, these notebook is also rendered on this website:<ul> <li>Tutorial notebook</li> </ul> </li> </ul>"}, {"location": "tutorials/#installation-instructions-for-active-projects", "title": "Installation Instructions for Active Projects", "text": "<ul> <li>The Element Facemap described above can be modified for a user's specific experimental requirements and thereby used in active projects.  </li> </ul> <ul> <li>The GitHub Codespace and Dev Container is configured for tutorials and prototyping. We recommend users to configure a database specifically for production pipelines.  Instructions for a local installation of the integrated development environment with a database can be found on the User Guide page.</li> </ul>"}]}